---
title: "semester_project"
output:
  word_document: default
  html_document: default
  pdf_document: default
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Statistical Learning Final Exam Project Mahir Selek, Joi Berberi July 26, 2022

NBA Players Salary Prediction using Multiple Regression Model

###1.Aim of the project 

As it is known, the NBA is one of today's biggest sports organizations, even one of the most popular. It is a huge organization that is regularly followed by so many people around the world every year, and beyond that it is a huge monetary market. As two sports enthusiast students, we decided to make a project about basketball players playing in this popular league. We wanted to do a forecasting project because it is a big league in terms of money and because the income wages of the players arouse curiosity every year. At the same time, this project should have been linked to the content of our course. That's why we decided it was appropriate for this project to do a regression project on players' statistics and income salaries.

###2.Import Libraries

```{r}
library(data.table)
library(corrplot)
library(GGally)
library(tidyverse)
library(PerformanceAnalytics)
library(plotly)
#library(magrittr) # needs to be run every time you start R and want to use %>%
library(dplyr)    # alternatively, this also loads %>%
library(ggplot2)
library(caTools)
library(MASS)
library(caret)
library(topicmodels)
library(magrittr) # needs to be run every time you start R and want to use %>%
library(dplyr) 
```


###3.Dataset Description


For this project we decided to use the "NBA Players stats since 1950" dataset which is available on Kaggle. However "player_salary" dataset was not provided at Kaggle. So We scraped from '<https://www.basketball-reference.com/contracts/players.html>' website and created by ourselves. The first dataset contains aggregate individual statistics for 67 NBA seasons since 1950. From basic box-score attributes such as points, assists, rebounds etc., to more advanced money-ball like features such as Value Over Replacement. However, unfortunately we didn't focus many of the attributes in the dataset because they were not useful for our project. For this reason we eliminate them in this way we were able to focus on the statistical analysis. We will show that later inside the project.

```{r}
salary.table <- read.csv("player_salary.csv")
seasons_stats <- read.csv("seasons_stats.csv")
players_info <- read.csv("Players.csv")
```


###3.1.Data Components

```{r}
head(players_info)
```

In this dataset, only the teams of the players and the salaries they earned in the 2017-2018 season are included.

```{r}
head(seasons_stats)
```
```{r}
names(seasons_stats)
seasons_stats
```

The reason for the NA values this dataset begins from 1950 and at the beginning some of the values are empty. These are the features that can not be NA because we can not give them a default value or the missing value compromised the future model.

-pos: position of the player 
-Tm: Team of the player 
-G: Players total game number during one season 
-GS: Games Started (available since the 1982 season) 
-MP: Total minutes of the player in one season 
-BLK: Total block count of the player in one season 
-AST: Total assist number of the player in one season 
-PTS: Total score of the player in one season 
-STL: Total steal number of the player in one season 
-TRB: Total Rebound number of the player in one season 
-eFG: Effective Field Goal Percentage; the formula is (FG + 0.5 \* 3P) / FGA. 
This statistic adjusts for the fact that a 3-point field goal is worth one more point than a 2-point field goal. For example, suppose Player A goes 4 for 10 with 2 threes, while Player B goes 5 for 10 with 0 threes. Each player would have 10 points from field goals, and thus would have the same effective field goal percentage (50%). -FGA: Field Goal Attempts (includes both 2-point field goal attempts and 3-point field goal attempts) 
-FT: Free Throws 
-2P: 2-Point Field Goals 
-2PA: 2-Point Field Goal Attempts 
-3P: 3-Point Field Goals (available since the 1979-80 season in the NBA) 
-DRB: Defensive Rebounds (available since the 1973-74 season in the NBA) ...

Other categories(columns) containing player data throughout the seasons. As we mentioned above we are not going to use many of them because of they had no value in our analysis.

##3.2.Data Cleaning and Filtering


```{r}

stats17 <- 
  seasons_stats %>% filter(Year >= 2017) %>% 
  dplyr::select(Year:G, MP, PER, FG:PTS) %>% 
  distinct(Player, .keep_all = TRUE) %>% 
  mutate(MPG = MP/G, PPG = PTS/G, APG = AST/G, 
         RPG = TRB/G, TOPG = TOV/G, BPG = BLK/G, 
        SPG = STL/G)


# players_info <- players_info()
#   players_info %>% filter(Year >= 2017) %>% 
#   dplyr::select(Year:G, MP, PER, FG:PTS) %>% 
#   distinct(Player, .keep_all = TRUE) %>% 
#   mutate(MPG = MP/G, PPG = PTS/G, APG = AST/G, 
#          RPG = TRB/G, TOPG = TOV/G, BPG = BLK/G, 
#         SPG = STL/G)


players_info <- (subset(players_info, select = c(seq(Player,weight))))

players_info

stats17

```

The advantage of the filtering after 2017 is that we don't have any NA or empty feature anymore. Thus, we have transformed our data into a more useful form. distinct functions helps us to retain only unique/distinct rows from our input tables. Aim of the mutation is that in the seasons_stats file we don't have stats per game features. So we mutated all of them to use in our salary prediction project. Our main purpose is to investigate how the stats effect next season's salary the players get.

##3.3.Data Merging

```{r}
#Merging Data
players_info

stats_salary <- merge(stats17, salary.table, by.x = "Player", by.y = "Player")
stats_salary


stats_salary <- merge(stats_salary, players_info, by.x = "Player", by.y = "Player")
# 
names(stats_salary)[40] <- "salary17_18"
stats_salary <- (subset(stats_salary, select = -c(X, Team)))

stats_salary
```

```{r}
stats_salary <- (subset(stats_salary, select = -c(seq(G,PTS))))

```

```{r}
glimpse(stats_salary)
```

Now as we can see our data become more clear and understandable.
We prefer to use specific data belongs only on 2017. 
Also we created new variables to predict better and understandable data. 
Why we choose these 7 parameter? Because these parameters are the numerical parameters on which we can predict what we need for our prediction model.


##4.Exploratory Data Analysis

The first thing we did was to look at the distributions of the continuous variables conditioned on the stats salary.

```{r}
cont.vars = subset(stats_salary,select=-c(Tm, Pos));
summary(cont.vars)
```

From this summary statistic of the data we noticed that there are some strange values, in particular some values of MPG, BPG and SPG seems too high from a medical point of view. To understand more clearer, we preferred to use correlation in order to look at the data we have from the outside. Being able to draw such a straight line helps us not only predict the unknown but also understand the relationship between the variables better. We also expect some variables to be correlated, it's better to check that before fitting some model. For continuous variables we checked the correlation plots and the correlation matrix to get a visual idea. Thanks to the correlation, it helped us to have an idea about which data would more closely affect our result.

```{r}
ggplot(data = stats_salary, mapping = aes(x = MPG, y=PPG, color = Pos, shape = Pos)) + 
  geom_point()
```

We used variable Pos in the dataset to separate the players into five groups and highlighted the points by different colors and shapes. As can be clearly seen, there is a situation that is scattered in a mixed state. We can't create any relationship with salary

4.1.Correlation Check

```{r}
#Correlation Check1

plot(subset(stats_salary,select = c(salary17_18,PPG,MPG,TOPG,RPG,SPG,APG,BPG,Age,height,weight)))


cor.mat=cor(subset(stats_salary,select = c(salary17_18,PPG,MPG,TOPG,RPG,SPG,APG,BPG,Age,height,weight)));
corrplot(cor.mat, method="color",addCoef.col = "black",tl.srt=30)
```

```{r}
#Correlation Check2
corrplot(cor(stats_salary %>% 
               dplyr::select(salary17_18, MPG:SPG, 
                      Age, height, weight, contains("%")), 
             use = "complete.obs"), 
         method = "circle",type = "upper")




#Another type of Correlation Check
stats_salary_cor <- 
  stats_salary %>% 
  dplyr::select(salary17_18, PPG, MPG, TOPG, RPG, SPG, APG, BPG, Age, height, weight)
ggpairs(stats_salary_cor)

```

```{r}
cor(stats_salary_cor)[,"salary17_18"]

```

The interesting part of this is that the number of turnover players make is linked to their salary, and the relationship has a positive correlation. So, We interpreted this relationship like this: "the more turnovers they make" means that they are more involved in ball movements in games, which means that players who make turnovers are, at some extend, important to their team. and we thought this could be expressed as "aggressiveness". We already know that this interpretation could not be appropriate one. We have use methods which we learn during classes at this point.

##4.1.Data Visualization

```{r}
#4.1 Interactive Plot

names(stats_salary)[5] <- "Team"
plot_ly(data = stats_salary, x = ~salary17_18, y = ~PPG, color = ~Team,
        hoverinfo = "text",
        text = ~paste("Player: ", Player,
                      "<br>Salary: ", format(salary17_18, big.mark = ","),"$",
                      "<br>PPG: ", round(PPG, digits = 3),
                      "<br>Team: ", Team)) %>% 
  layout(
    title = "Salary vs Point Per Game",
    xaxis = list(title = "Salary USD"),
    yaxis = list(title = "Point per Game")
  )

```

Why we used Point Per Game? Because after analyze PPG is the most correlated variable for us. PPG is directly effects on salary. It is evident that the value of the PPG associated to Salary is much larger than the others. Clearly, we can see that in this visualization

```{r}
par(mfrow=c(2,4))

boxplot(stats_salary$PPG, col='lightsalmon1')
boxplot(stats_salary$MPG, col='lightsalmon1')
boxplot(stats_salary$TOPG, col='lightsalmon1')
boxplot(stats_salary$RPG, col='lightsalmon1')
boxplot(stats_salary$SPG, col='lightsalmon1')
boxplot(stats_salary$APG, col='lightsalmon1')
boxplot(stats_salary$BPG, col='lightsalmon1')
boxplot(stats_salary$Age, col='lightsalmon1')
boxplot(stats_salary$height, col='lightsalmon1')
boxplot(stats_salary$weight, col='lightsalmon1')


```

We look to the features independently from the boxplot it can be noticed that the most frequent feature is MPG, as expected APG and BPG rappresent a minority.

```{r}
# season-related attributes 17/18
par(mfrow=c(2,2))

hist(stats_salary$PPG, freq = FALSE,main = "PPG", col="lightcoral")
lines(density(stats_salary$PPG))

hist(stats_salary$MPG, freq = FALSE,main = "MPG", col="lightcoral")
lines(density(stats_salary$MPG))

hist(stats_salary$TOPG, freq = FALSE,main = "RPG", col="lightcoral")
lines(density(stats_salary$TOPG))

hist(stats_salary$RPG, freq = FALSE,main = "TOPG", col="lightcoral")
lines(density(stats_salary$RPG))

hist(stats_salary$SPG, freq = FALSE,main = "SPG", col="lightcoral")
lines(density(stats_salary$SPG))

hist(stats_salary$APG, freq = FALSE,main = "APG", col="lightcoral")
lines(density(stats_salary$APG))

hist(stats_salary$BPG, freq = FALSE,main = "BPG", col="lightcoral")
lines(density(stats_salary$BPG))

hist(stats_salary$Age, freq = FALSE,main = "Age", col="lightcoral")
lines(density(stats_salary$Age))

hist(stats_salary$Age, freq = FALSE,main = "height", col="lightcoral")
lines(density(stats_salary$height))

hist(stats_salary$Age, freq = FALSE,main = "weight", col="lightcoral")
lines(density(stats_salary$weight))



# comparing relationship different independent variables

qqnorm(stats_salary$PPG,main = "PPG")
qqline(stats_salary$PPG)
qqnorm(stats_salary$MPG,main = "MPG")
qqline(stats_salary$MPG)
qqnorm(stats_salary$TOPG,main = "TOPG")
qqline(stats_salary$TOPG)
qqnorm(stats_salary$RPG,main = "RPG")
qqline(stats_salary$RPG)
qqnorm(stats_salary$SPG,main = "SPG")
qqline(stats_salary$SPG)
qqnorm(stats_salary$APG,main = "APG")
qqline(stats_salary$APG)
qqnorm(stats_salary$BPG,main = "BPG")
qqline(stats_salary$BPG)
qqnorm(stats_salary$Age,main = "Age")
qqline(stats_salary$Age)
qqnorm(stats_salary$height,main = "height")
qqline(stats_salary$height)
qqnorm(stats_salary$weight,main = "weight")
qqline(stats_salary$weight)


```

The plot confirms what we already said above: the variables related to the salary pairly show an evident linear dependence. Moreover, we can observe that there exists also a relationship between salary and independent variables: -Between TOPG and salary has a positive correlation.

When the response variable is right skewed, many think regression becomes difficult. Skewed data is generally thought of as problematic. At this point, we need to think about MPG. Because the disadvantage of this variable is that it manipulates our model. During an NBA game, 5 active players have to stay on the field. This means that players with bad or average statistics are constantly on the field. Therefore, we cannot rely on this variable and it does not give us healthy information. There may be players who are played for tactical purposes only, taking too much time, which is not useful for us.

Briefly, The Normal QQ plot is used to evaluate how well the distribution of a dataset matches a standard normal (Gaussian) distribution. MPG one that more uniform distribution but still we have some doubts about this variable. In addition, we will re-consider this variable during the model building phase for the reason explained above. Almost every variable has good relationship with our dependent variable at this point except Assist Per Game and Block Per Game.



##5.Multiple Regression

For almost the entire project, we decided to focus the analyzes on the salary.

in classic linear regression we have to see firstly how the covariate/explanatory variable are able to estimate our response dependent function salary, then in 
multiple regression we can see how much these exaplanatory variable working with each other are good to estimate our Y function.
for seeing the effectivness of these variable we can use the correlation function for make sure how correlated are our explanatory variables.
I look the effectect of one variable when the other is fixed


in linear model:
1. function should be linear
2. homoscedasticity = variance is the same for all the error
3. error term is normal and independence


To analyze in detail the results obtained from this first simple model, we recall the necessary theoretical assumptions that must be verified. Checking model assumptions is essential prior to building a model that will be used for prediction. If our assumptions are not met, the model may inaccurately reflect the data and will likely result in inaccurate predictions.

Assessing Model Assumptions

The model assumptions are: 
- Linearity of the response-predictor relationships;
- Homoscedasticity of the error terms: Var($\epsilon_i$) = $\sigma^2$. homogeneity of variances.
- Normality and independence of the error terms we are adding on exploratory variables. Exploratory variables are our predictors.


-Linearity

The plot of residuals versus fitted values shows a little pattern, however it seems to indicate that there are linear associations in the data and that the errors are uncorrelated.

-Homoschedasticity

The presence of a funnel shape in the residual plot suggests that the error terms do not have a constant variance. One possible solution is to transform the response variable $Y$ using a concave function such as $\log(Y)$ or $\sqrt{Y}$.

With a trial and error approach we came out with the best transformation: $\sqrt(Y)$, of which the resulting density is shown the following figures.


##5.2. MODEL TEST

```{r, fig.height=4, fig.width=8}
par(mfrow=c(1,3))
hist(stats_salary$salary17_18, breaks=30,main="Annual Salary Population",col="#BBDFFF",
     xlab = "salary17_18")
lines(density(stats_salary$salary17_18))


hist(log(stats_salary$salary17_18),  breaks=30, col="#BBDFFF", main="log(Annual Salary)", xlab = "log(salary17_18)")
lines(density(log(stats_salary$salary17_18)))


hist(sqrt(stats_salary$salary17_18),  breaks=30, col="#BBDFFF", main="sqrt(Annual Salary)", xlab = "sqrt(salary17_18)")
lines(density(sqrt(stats_salary$salary17_18)))

par(mfrow=c(1,1))
```

From this histograms we can see that when we use Square Root Transform instead of Log Transform, we got slightly better results. First we tried log transform but the results were not satisfactory. Actually, Log transformation is most likely the first thing you should do to remove skewness from the predictor. After that we used squared root transform, which gives us a distribution more similar to the normal distribuition


```{r, fig.height=4, fig.width=8}
par(mfrow=c(1,3))
qqnorm(stats_salary$salary17_18, main="Annual Salary")
qqline(stats_salary$salary17_18)
qqnorm(log(stats_salary$salary17_18), main="log(Annual Salary)")
qqline(log(stats_salary$salary17_18))
qqnorm(sqrt(stats_salary$salary17_18), main="sqrt(Annual Salary)")
qqline(sqrt(stats_salary$salary17_18))
par(mfrow=c(1,1))
```
To check the conditions listed above we use the residual plots provided by lm.(The lm() function is used to fit linear models to data frames in the R Language.).

#TEST 1.1

Residuals vs Fitted:

If the residuals not randomly distributed around x axis -> value of the residual should be near to 0.

Residuals vs Fitted: we expect to have points randomly dispersed around x axis 
qq: to assess normality of residuals 
leverage: low is the leverage, better our predictors should work. if there is high leverage values they have bad effect on our model. They are acting like a outliers.

```{r}
# player attributes 

lm.model1 <- lm(formula= salary17_18 ~ MPG+PPG+APG+RPG+TOPG+BPG+SPG+Age+height+weight, data=stats_salary) 
summary(lm.model1) #Adjusted R-squared:  0.5639

# visualize model results
par(mfrow=c(2,2))
plot(lm.model1)
mtext("Model 1: Skewed Probability Distribution", side = 3, line = -28, outer = TRUE)
par(mfrow=c(1,1))
```

#TEST 1.2 Convert response variable to log

```{r}
stats_salary$salary17_18_log <- log(stats_salary$salary17_18)
lm.model_log <- lm(formula= salary17_18_log ~  MPG+PPG+APG+RPG+TOPG+BPG+SPG+Age+height+weight, data=stats_salary)
summary(lm.model_log) #Adjusted R-squared:  0.4708  

par(mfrow=c(2,2))
plot(lm.model_log)
mtext("Model 2: Log-transformed distribution", side = 3, line = -28, outer = TRUE)
par(mfrow=c(1,1))
```


#TEST 1.3 Convert response variable to sqrt

```{r}
stats_salary$salary17_18_sqrt <- sqrt(stats_salary$salary17_18)
lm.model_sqrt <- lm(formula= salary17_18_sqrt ~  MPG+PPG+APG+RPG+TOPG+BPG+SPG+Age+height+weight, data=stats_salary)
summary(lm.model_sqrt) #Adjusted R-squared:  0.5757   

par(mfrow=c(2,2))
plot(lm.model_sqrt)
mtext("Model 3: Square Root-transformed distribution", side = 3, line = -28, outer = TRUE)
par(mfrow=c(1,1))
```

As we can see our adjusted R-squared is near to 1 (is near to normal distribution) with the square root distribution so i will use it in our project

R-Square: measures the proportion of variability in y that can be explained using x,
aim is to make r square near to one -> measures of how regression predictions approximate real data points
if it's = to 1 means that RSS is equal to 0 (residual sum of squared) it means that our regression predictions fit very well the data




###5.3.BACKWARD MODEL SELECTION

In order to understand which feature to keep we apply "backward selection" on the model

#TEST 2 removing "BPG" predictor

```{r}
lm.model_sqrt <- lm(formula= salary17_18_sqrt ~  MPG+PPG+APG+RPG+TOPG+SPG+Age+height+weight, data=stats_salary)
summary(lm.model_sqrt) #Adjusted R-squared:  0.5764   

par(mfrow=c(2,2))
plot(lm.model_sqrt)
mtext("Model 3: MPG+PPG+APG+RPG+TOPG+SPG+Age+height+weight", side = 3, line = -28, outer = TRUE)
par(mfrow=c(1,1))
```

#TEST 3 removing "weight" predictor

```{r}
lm.model_sqrt <- lm(formula= salary17_18_sqrt ~  MPG+PPG+APG+RPG+TOPG+SPG+Age+height, data=stats_salary)
summary(lm.model_sqrt) #Adjusted R-squared:  0.5767

par(mfrow=c(2,2))
plot(lm.model_sqrt)
mtext("Model 1: MPG+PPG+APG+RPG+TOPG+SPG+Age+height", side = 3, line = -28, outer = TRUE)
par(mfrow=c(1,1))
```


#TEST 4 removing "SPG" predictor to obtaining BEST MODEL

```{r}
lm.model_sqrt <- lm(formula= salary17_18_sqrt ~  MPG+PPG+APG+RPG+TOPG+Age+height, data=stats_salary)
summary(lm.model_sqrt) #Adjusted R-squared:  0.5767

par(mfrow=c(2,2))
plot(lm.model_sqrt)
mtext("Model 3: MPG+PPG+APG+RPG+TOPG+Age+height", side = 3, line = -28, outer = TRUE)
par(mfrow=c(1,1))
```


Coefficients

```{r}
coefficients(lm.model_sqrt)
```

Other Possible Problems

Outliers

The residual plot identifies some outliers. However, it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals, computed by dividing each residual $e_i$ by its estimated standard error. Observations whose studentized residuals are greater than 3 in absolute value are possible outliers.

Note that the empirical motivation for the value equal to 3 is that the Standardized Residuals are approximated by a $N(0,1)$. The probability to observe a value greater than 3 is then 0.001349898[1].

```{r}
1-pnorm(3)
```

This norm is the value to identify that the pnorm in R is a built-in function that returns the value of the cumulative density function (cdf) of the normal distribution given a certain random variable q, and a population mean μ, and the population standard deviation σ.


An outlier is a data point whose response y does not follow the general trend of the rest of the data. A data point has high leverage if it has "extreme" predictor x values. With a single predictor, an extreme x value is simply one that is particularly high or low.

a studentized residual is the value resulting from the division of a residual by an estimate of its standard deviation. It is a form of a Student's t-statistic, with the estimate of error varying between points. This is an important technique in the detection of outliers.

```{r, fig.height=4, fig.width=6}
plot(predict(lm.model_sqrt), rstandard(lm.model_sqrt), xlab="Fitted Values", ylab = "Studentized Residuals")
abline(h=3, col = "red")
abline(h=-3, col = "red")
```

```{r}
par(mfrow=c(2,2))
plot(lm.model_sqrt)
mtext("Model 5.2: Final reduced model", side = 3, line = -28, outer = TRUE)
par(mfrow=c(1,1))


out <- names(rstandard(lm.model_sqrt)[(abs(rstandard(lm.model_sqrt)) > 3)])

# we have 442 players
playerout<-stats_salary$Player[rownames(stats_salary) %in% out]

# player I want to remove that rappresent my outliers
playerout

```

High Leverage Points

A second problem when dealing with regression is the presence of high leverage points. In order to quantify an observations leverage, we compute the leverage statistic. If a given observation has a leverage statistic that greatly exceeds $(p+1)/n$, then we may suspect that the corresponding point has high leverage.


```{r}
# REMOVE HLP (HIGH LEVERAGE POINTS)
# removing the High leverage points doesn't not improve the model, 
#indeed it worsens our estimate of R squared = 0.5565
stats_salary_copy <- stats_salary

dim(stats_salary_copy)[1]-length(hatvalues(lm.model_sqrt))
total.length <- dim(stats_salary_copy)[1]
stats_salary_copy <- stats_salary_copy[hatvalues(lm.model_sqrt) <= 4 * mean(hatvalues(lm.model_sqrt)),]
total.length-dim(stats_salary_copy)[1] # removed high leverage points


# lm.model_sqrt_HLP without High leverage points
lm.model_sqrt_HLP <- lm(formula= salary17_18_sqrt ~  MPG+PPG+APG+RPG+TOPG+Age, data=stats_salary_copy)
summary(lm.model_sqrt_HLP) #Adjusted R-squared:  53.82%

```

```{r}

#hv <- hatvalues(lm.model_sqrt_HLP)
#plot(hv, rstandard(lm.model_sqrt_HLP), xlab="Leverage", ylab = "Studentized Residuals")


#p <- dim(stats_salary)[2]-1
#n <- dim(stats_salary)[1]
#abline(v=(p+1)/n, col = "red")
```

Visualize model results



###OTHER MODELS:
```{r}

################################# REGRESSION ON TRAINING SET #################################

### CREATE TRAIN/TEST SPLIT ###
library(MASS)
library(caTools)
library(caret)


set.seed(100)
split = sample.split(stats_salary$salary17_18_sqrt, SplitRatio = 0.75)

training = subset(stats_salary, split == TRUE)
testing = subset(stats_salary, split == FALSE)


# here we use the reduced best model (last tested) with the selected features only (after backward model selection)
lm.model <- lm(formula= salary17_18_sqrt ~
                 MPG+PPG+APG+RPG+TOPG,
               data=training)
summary(lm.model) # ~ Adjusted R-squared: 55.33%
# visualize model results
par(mfrow=c(2,2))
plot(lm.model)
par(mfrow=c(1,1))
```

```{r}

################################# PREDICTION ON TEST SET #################################

## use the predicted values of the model and save a new feature into the model
testing['salary17_18_sqrt_predicted'] <- predict(lm.model, newdata = testing)

# simple visualization of how the two distributions overlap
hist(testing$salary17_18_sqrt_predicted, freq = FALSE, col=(rgb(220,20,60, max = 255, alpha = 100)), main="Predicted vs Salary value")

hist(testing$salary17_18_sqrt,  freq = FALSE,add=T, col=(rgb(70,130,180, max = 255, alpha = 100)))
# as we can see, the prediction is more concentrated around the mean and has lower tails

summary(testing$salary17_18_sqrt)-summary(testing$salary17_18_sqrt_predicted) # low values => similar around the mean, diverge at the extremes

#red we present the predicted salary values and in blue we shower are our true salary values

```

```{r}

################################# CLASSIFICATION #################################

get.accuracy <- function(training,testing,percentages,salary_class_names){
  quantiles <- quantile(training$salary17_18_sqrt,percentages)
  print("Percentiles used:")
  print(quantiles)
  N <- length(quantiles)+1
  # create the "salary_class" feature for the test set
  for(i in 1:N){
    if(i==1){
      testing[testing$salary17_18_sqrt<=quantiles[[i]],'salary_class']=salary_class_names[i]
      testing[testing$salary17_18_sqrt_predicted<=quantiles[[i]],'salary_class_predicted']=salary_class_names[i]
    }
    else if (i==N){
      testing[(testing$salary17_18_sqrt>quantiles[[i-1]]),'salary_class']=salary_class_names[i]
      testing[(testing$salary17_18_sqrt_predicted>quantiles[[i-1]]),'salary_class_predicted']=salary_class_names[i]
    }
    else{
      testing[(testing$salary17_18_sqrt>quantiles[[i-1]])&(testing$salary17_18_sqrt<=quantiles[[i]]),'salary_class']=salary_class_names[i]
      testing[(testing$salary17_18_sqrt_predicted>quantiles[[i-1]])&(testing$salary17_18_sqrt_predicted<=quantiles[[i]]),'salary_class_predicted']=salary_class_names[i]
    }
  }

  testing$salary_class <- as.factor(testing$salary_class)
  testing$salary_class_predicted <- as.factor(testing$salary_class_predicted)

  par(mfrow=c(1,2))
  plot(testing$salary_class,col=(rgb(0,0,255, max = 255, alpha = 100)), main="Salary Class")
  plot(testing$salary_class_predicted,col=(rgb(255,0,0, max = 255, alpha = 100)),main="Salary Class Predicted")
  par(mfrow=c(1,1))


  ### build the confusion matrix by hand
  salary_class <- salary_class_names
  cm <- matrix(0,nrow=N,ncol=N)
  rownames(cm) <- salary_class_names
  colnames(cm) <- salary_class_names
  for(p1 in salary_class){
    for(p2 in salary_class){
      cm[p1,p2] <- dim(testing[(testing$salary_class==p1) & (testing$salary_class_predicted==p2),])[1]
    }
  }

  # visualize results
  print(paste("Confusion Matrix (", dim(testing)[1] ,"istances )"))
  print(cm)
  # calculate accuracy
  print(paste("Accuracy =",round(sum(diag(cm))/sum(cm)*10000)/100,"%"))
}

# with 3 classes divided using 33° and 66° percentile of the salary_class distribution
get.accuracy(training,testing,c(.33,.67),c("low","medium","high")) # Accuracy 63.49%
```


```{r}

## LDA BUILT-IN FUNCTION Linear discriminant analysis

thresholds <- c(.33,.67)
class.names <- c("low","medium","high")


q <- quantile(stats_salary$salary17_18_sqrt,thresholds)

# create salary_class feature on entire dataset based on percentile thresholds
stats_salary[stats_salary$salary17_18_sqrt<=q[[1]],'salary_class']=class.names[1]
stats_salary[(stats_salary$salary17_18_sqrt>q[[1]])&(stats_salary$salary17_18_sqrt<=q[[2]]),'salary_class']=class.names[2]
stats_salary[stats_salary$salary17_18_sqrt>q[[2]],'salary_class']=class.names[3]

stats_salary$salary_class <- as.factor(stats_salary$salary_class)
plot(stats_salary$salary_class, main="Distribution of 3 'salary_class' among entire dataset")

# same seed as before to be consistent with the train/test split
set.seed(100)
split = sample.split(stats_salary$salary17_18_sqrt, SplitRatio = 0.75)

training = subset(stats_salary, split == TRUE)
testing = subset(stats_salary, split == FALSE)

# Here we use the same features but the model tries to fit on the "salary_class" feature directly
lda.model <- lda(formula=salary_class~
                  MPG+PPG+APG+RPG+TOPG,
                data=training)

confusionMatrix(testing$salary_class, predict(lda.model,newdata = testing)$class) # Accuracy 71.17%


# Here we use the same features but the model tries to fit on the "salary_class" feature directly
qda.model <- qda(formula=salary_class~
                   MPG+PPG+APG+RPG+TOPG,
                data=training)

confusionMatrix(testing$salary_class, predict(qda.model,newdata = testing)$class) # Accuracy  58.56%

```


DIFFERENCES BETWEEN QUADRATIC AND LINEAR 

The answer is simplicity of the model. Our model is simple and LDA gives us better result for that reason. Our accuracy increased Our sensitivity is increased.

###6.CONCLUSION(Agressiveness and Trusted)

```{r}
MPG.value <- stats_salary$MPG
TOPG.value <- stats_salary$TOPG


avg.minutes <- mean(MPG.value)
avg.turnover <- mean(TOPG.value)

stats_salary[stats_salary$salary17_18_sqrt<=q[[1]],'salary_class']=class.names[1]

stats_salary$Trusted <- as.factor(ifelse(MPG.value >= avg.minutes, "Yes", "No"))
stats_salary$Agressiveness <- as.factor(ifelse(TOPG.value >= avg.turnover, "Yes", "No"))
head(stats_salary)

```
#Data Visualization

```{r}

#install.packages("magrittr") # package installations are only needed the first time you use it
#install.packages("dplyr")    # alternative installation of the %>%
#library(magrittr) # needs to be run every time you start R and want to use %>%
#library(dplyr) 

stats_salary %>% 
  ggplot(aes(x = salary17_18_sqrt, y = PPG, colour = Agressiveness)) + 
  geom_point() + 
  geom_smooth(method='lm')


lm(formula = salary17_18 ~ Trusted * Agressiveness, data=stats_salary)
```
